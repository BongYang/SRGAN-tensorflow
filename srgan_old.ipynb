{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "if gpus:\n",
    "  # 텐서플로가 첫 번째 GPU만 사용하도록 제한\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    except RuntimeError as e:\n",
    "    # 프로그램 시작시에 접근 가능한 장치가 설정되어야만 합니다\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import scipy\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D, Add\n",
    "from tensorflow.keras.layers import PReLU, LeakyReLU\n",
    "from tensorflow.keras.layers import UpSampling2D, Conv2D\n",
    "\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from data_loader import DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow import keras\n",
    "\n",
    "tf.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.channels = 3\n",
    "        self.lr_height = 64                 # Low resolution height\n",
    "        self.lr_width = 64                  # Low resolution width\n",
    "        self.lr_shape = (self.lr_height, self.lr_width, self.channels)\n",
    "        self.hr_height = self.lr_height*4   # High resolution height\n",
    "        self.hr_width = self.lr_width*4     # High resolution width\n",
    "        self.hr_shape = (self.hr_height, self.hr_width, self.channels)\n",
    "\n",
    "        # Number of residual blocks in the generator\n",
    "        self.n_residual_blocks = 16\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # We use a pre-trained VGG19 model to extract image features from the high resolution\n",
    "        # and the generated high resolution images and minimize the mse between them\n",
    "        self.vgg = self.build_vgg()\n",
    "        self.vgg.trainable = False\n",
    "        self.vgg.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Configure data loader\n",
    "        self.dataset_name = 'img_align_celeba'\n",
    "        self.data_loader = DataLoader(dataset_name=self.dataset_name,\n",
    "                                      img_res=(self.hr_height, self.hr_width))\n",
    "        \n",
    "        # Calculate output shape of D (PatchGAN)\n",
    "        patch = int(self.hr_height / 2**4)\n",
    "        self.disc_patch = (patch, patch, 1)\n",
    "\n",
    "        # Number of filters in the first layer of G and D\n",
    "        self.gf = 64\n",
    "        self.df = 64\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # High res. and low res. images\n",
    "        img_hr = Input(shape=self.hr_shape)\n",
    "        img_lr = Input(shape=self.lr_shape)\n",
    "\n",
    "        # Generate high res. version from low res.\n",
    "        fake_hr = self.generator(img_lr)\n",
    "\n",
    "        # Extract image features of the generated img\n",
    "        fake_features = self.vgg(fake_hr)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # Discriminator determines validity of generated high res. images\n",
    "        validity = self.discriminator(fake_hr)\n",
    "\n",
    "        self.combined = Model([img_lr, img_hr], [validity, fake_features])\n",
    "        self.combined.compile(loss=['binary_crossentropy', 'mse'],\n",
    "                              loss_weights=[1e-3, 1],\n",
    "                              optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def build_vgg(self):\n",
    "        \"\"\"\n",
    "        Builds a pre-trained VGG19 model that outputs image features extracted at the\n",
    "        third block of the model\n",
    "        \"\"\"\n",
    "        vgg19 = VGG19(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=self.hr_shape))\n",
    "        # Set outputs to outputs of last conv. layer in block 3\n",
    "        # See architecture at: https://github.com/keras-team/keras/blob/master/keras/applications/vgg19.py\n",
    "#         vgg.outputs = [vgg.layers[9].output]\n",
    "        \n",
    "#         vgg.summary()\n",
    "#         img = Input(shape=self.hr_shape)\n",
    "#         print(img.shape)\n",
    "        \n",
    "        # Extract image features\n",
    "#         img_features = vgg(img)\n",
    "\n",
    "#         return Model(img, img_features)\n",
    "        return Model(inputs=vgg19.input, outputs=vgg19.get_layer('block3_conv4').output)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        def residual_block(layer_input, filters):\n",
    "            \"\"\"Residual block described in paper\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=3, strides=1, padding='same')(layer_input)\n",
    "            d = Activation('relu')(d)\n",
    "            d = BatchNormalization(momentum=0.8)(d)\n",
    "            d = Conv2D(filters, kernel_size=3, strides=1, padding='same')(d)\n",
    "            d = BatchNormalization(momentum=0.8)(d)\n",
    "            d = Add()([d, layer_input])\n",
    "            return d\n",
    "\n",
    "        def deconv2d(layer_input):\n",
    "            \"\"\"Layers used during upsampling\"\"\"\n",
    "            u = UpSampling2D(size=2)(layer_input)\n",
    "            u = Conv2D(256, kernel_size=3, strides=1, padding='same')(u)\n",
    "            u = Activation('relu')(u)\n",
    "            return u\n",
    "\n",
    "        # Low resolution image input\n",
    "        img_lr = Input(shape=self.lr_shape)\n",
    "\n",
    "        # Pre-residual block\n",
    "        c1 = Conv2D(64, kernel_size=9, strides=1, padding='same')(img_lr)\n",
    "        c1 = Activation('relu')(c1)\n",
    "\n",
    "        # Propogate through residual blocks\n",
    "        r = residual_block(c1, self.gf)\n",
    "        for _ in range(self.n_residual_blocks - 1):\n",
    "            r = residual_block(r, self.gf)\n",
    "\n",
    "        # Post-residual block\n",
    "        c2 = Conv2D(64, kernel_size=3, strides=1, padding='same')(r)\n",
    "        c2 = BatchNormalization(momentum=0.8)(c2)\n",
    "        c2 = Add()([c2, c1])\n",
    "\n",
    "        # Upsampling\n",
    "        u1 = deconv2d(c2)\n",
    "        u2 = deconv2d(u1)\n",
    "\n",
    "        # Generate high resolution output\n",
    "        gen_hr = Conv2D(self.channels, kernel_size=9, strides=1, padding='same', activation='tanh')(u2)\n",
    "\n",
    "        return Model(img_lr, gen_hr)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        def d_block(layer_input, filters, strides=1, bn=True):\n",
    "            \"\"\"Discriminator layer\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=3, strides=strides, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if bn:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            return d\n",
    "\n",
    "        # Input img\n",
    "        d0 = Input(shape=self.hr_shape)\n",
    "\n",
    "        d1 = d_block(d0, self.df, bn=False)\n",
    "        d2 = d_block(d1, self.df, strides=2)\n",
    "        d3 = d_block(d2, self.df*2)\n",
    "        d4 = d_block(d3, self.df*2, strides=2)\n",
    "        d5 = d_block(d4, self.df*4)\n",
    "        d6 = d_block(d5, self.df*4, strides=2)\n",
    "        d7 = d_block(d6, self.df*8)\n",
    "        d8 = d_block(d7, self.df*8, strides=2)\n",
    "\n",
    "        d9 = Dense(self.df*16)(d8)\n",
    "        d10 = LeakyReLU(alpha=0.2)(d9)\n",
    "        validity = Dense(1, activation='sigmoid')(d10)\n",
    "\n",
    "        return Model(d0, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=1, sample_interval=50):\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ----------------------\n",
    "            #  Train Discriminator\n",
    "            # ----------------------\n",
    "\n",
    "            # Sample images and their conditioning counterparts\n",
    "            imgs_hr, imgs_lr = self.data_loader.load_data(batch_size)\n",
    "            \n",
    "            # From low res. image generate high res. version\n",
    "            fake_hr = self.generator.predict(imgs_lr)\n",
    "\n",
    "            valid = np.ones((batch_size,) + self.disc_patch)\n",
    "            fake = np.zeros((batch_size,) + self.disc_patch)\n",
    "            \n",
    "            # Train the discriminators (original images = real / generated = Fake)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs_hr, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(fake_hr, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ------------------\n",
    "            #  Train Generator\n",
    "            # ------------------\n",
    "            \n",
    "            # Sample images and their conditioning counterparts\n",
    "            imgs_hr, imgs_lr = self.data_loader.load_data(batch_size)\n",
    "\n",
    "            # The generators want the discriminators to label the generated images as real\n",
    "            valid = np.ones((batch_size,) + self.disc_patch)\n",
    "\n",
    "            # Extract ground truth image features using pre-trained VGG19 model\n",
    "            image_features = self.vgg.predict(imgs_hr)\n",
    "\n",
    "            # Train the generators\n",
    "            g_loss,_,_ = self.combined.train_on_batch([imgs_lr, imgs_hr], [valid, image_features])\n",
    "            \n",
    "            \n",
    "            elapsed_time = datetime.datetime.now() - start_time\n",
    "            # Plot the progress\n",
    "            if epoch % 10 == 0:\n",
    "                print (\"%d time: %s g_loss: %s d_loss: %s\" % (epoch, elapsed_time, g_loss, d_loss[0]))\n",
    "                \n",
    "#             If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "            \n",
    "            if epoch == 3000:\n",
    "                self.generator.save('./saved_model/teacher_epoch3000')\n",
    "            \n",
    "    def sample_images(self, epoch):\n",
    "        os.makedirs('images/%s' % self.dataset_name, exist_ok=True)\n",
    "        r, c = 2, 2\n",
    "\n",
    "        imgs_hr, imgs_lr = self.data_loader.load_data(batch_size=2, is_testing=True)\n",
    "        fake_hr = self.generator.predict(imgs_lr)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        imgs_lr = 0.5 * imgs_lr + 0.5\n",
    "        fake_hr = 0.5 * fake_hr + 0.5\n",
    "        imgs_hr = 0.5 * imgs_hr + 0.5\n",
    "\n",
    "        # Save generated images and the high resolution originals\n",
    "        titles = ['Generated', 'Original']\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for row in range(r):\n",
    "            for col, image in enumerate([fake_hr, imgs_hr]):\n",
    "                axs[row, col].imshow(image[row])\n",
    "                axs[row, col].set_title(titles[col])\n",
    "                axs[row, col].axis('off')\n",
    "            cnt += 1\n",
    "        fig.savefig(\"images/%s/%d.png\" % (self.dataset_name, epoch))\n",
    "        plt.close()\n",
    "\n",
    "        # Save low resolution images for comparison\n",
    "        for i in range(r):\n",
    "            fig = plt.figure()\n",
    "            plt.imshow(imgs_lr[i])\n",
    "            fig.savefig('images/%s/%d_lowres%d.png' % (self.dataset_name, epoch, i))\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 time: 0:00:09.022362 g_loss: 391.3881530761719 d_loss: 0.3115745559334755\n",
      "10 time: 0:00:22.582775 g_loss: 143.034423828125 d_loss: 0.036898551508784294\n",
      "20 time: 0:00:34.996710 g_loss: 151.3246307373047 d_loss: 0.11423621326684952\n",
      "30 time: 0:00:47.345715 g_loss: 206.63092041015625 d_loss: 0.0080098154139705\n",
      "40 time: 0:00:59.735214 g_loss: 107.7813949584961 d_loss: 0.02149177179671824\n",
      "50 time: 0:01:12.103327 g_loss: 96.79265594482422 d_loss: 0.44683945178985596\n",
      "60 time: 0:01:25.404711 g_loss: 88.2862319946289 d_loss: 0.03172209765762091\n",
      "70 time: 0:01:37.733550 g_loss: 87.76997375488281 d_loss: 0.29922191936930176\n",
      "80 time: 0:01:50.026630 g_loss: 76.18598175048828 d_loss: 0.035597119480371475\n",
      "90 time: 0:02:02.315358 g_loss: 67.90010070800781 d_loss: 0.0013631187612190843\n",
      "100 time: 0:02:14.607920 g_loss: 63.84009552001953 d_loss: 0.04158050729893148\n",
      "110 time: 0:02:27.849296 g_loss: 79.8370132446289 d_loss: 0.01905744755640626\n",
      "120 time: 0:02:40.091827 g_loss: 111.9331283569336 d_loss: 0.0028967016842216253\n",
      "130 time: 0:02:52.322019 g_loss: 62.130958557128906 d_loss: 0.019422452431172132\n",
      "140 time: 0:03:04.559372 g_loss: 92.52259826660156 d_loss: 0.19159031473100185\n",
      "150 time: 0:03:16.870131 g_loss: 45.762855529785156 d_loss: 0.0069219996221363544\n",
      "160 time: 0:03:30.150578 g_loss: 56.78375244140625 d_loss: 0.0031461711041629314\n",
      "170 time: 0:03:42.420095 g_loss: 50.48019790649414 d_loss: 0.02486848016269505\n",
      "180 time: 0:03:54.794186 g_loss: 27.47964096069336 d_loss: 0.0148141750250943\n",
      "190 time: 0:04:07.109767 g_loss: 58.62091827392578 d_loss: 0.2775504161400022\n",
      "200 time: 0:04:19.388506 g_loss: 66.96381378173828 d_loss: 0.0955076715326868\n",
      "210 time: 0:04:32.586726 g_loss: 48.93997573852539 d_loss: 0.12863775773439556\n",
      "220 time: 0:04:44.924903 g_loss: 31.146940231323242 d_loss: 0.006558688823133707\n",
      "230 time: 0:04:57.129483 g_loss: 44.536376953125 d_loss: 0.3884015790099511\n",
      "240 time: 0:05:09.386209 g_loss: 39.634822845458984 d_loss: 0.011640748009085655\n",
      "250 time: 0:05:21.589718 g_loss: 33.66081619262695 d_loss: 0.06946149095892906\n",
      "260 time: 0:05:34.706551 g_loss: 36.47330093383789 d_loss: 0.04966542508918792\n",
      "270 time: 0:05:46.944199 g_loss: 45.2213134765625 d_loss: 0.004272102611139417\n",
      "280 time: 0:05:59.134442 g_loss: 22.190757751464844 d_loss: 0.30255920776107814\n",
      "290 time: 0:06:11.353929 g_loss: 50.22748565673828 d_loss: 0.21612834092229605\n",
      "300 time: 0:06:23.560252 g_loss: 40.1161003112793 d_loss: 0.11906494945287704\n",
      "310 time: 0:06:36.865402 g_loss: 30.314266204833984 d_loss: 0.3852073191665113\n",
      "320 time: 0:06:49.082492 g_loss: 30.864707946777344 d_loss: 0.003763101529330015\n",
      "330 time: 0:07:01.300114 g_loss: 58.84102249145508 d_loss: 0.2719648480415344\n",
      "340 time: 0:07:13.504285 g_loss: 59.39604187011719 d_loss: 0.48640889491070993\n",
      "350 time: 0:07:25.711145 g_loss: 21.399885177612305 d_loss: 0.06042284838622436\n",
      "360 time: 0:07:38.898788 g_loss: 37.38434600830078 d_loss: 0.008398292353376746\n",
      "370 time: 0:07:51.118224 g_loss: 27.596214294433594 d_loss: 0.006018823361955583\n",
      "380 time: 0:08:03.418474 g_loss: 32.87321472167969 d_loss: 0.46341701596975327\n",
      "390 time: 0:08:15.707148 g_loss: 61.59658432006836 d_loss: 0.08538535609841347\n",
      "400 time: 0:08:27.925604 g_loss: 27.347652435302734 d_loss: 0.010774715730804019\n",
      "410 time: 0:08:41.102599 g_loss: 61.053497314453125 d_loss: 0.14235399663448334\n",
      "420 time: 0:08:53.299972 g_loss: 18.285049438476562 d_loss: 0.06738415965810418\n",
      "430 time: 0:09:05.487815 g_loss: 29.504480361938477 d_loss: 0.0033541369484737515\n",
      "440 time: 0:09:17.639588 g_loss: 28.61700439453125 d_loss: 0.05592109658755362\n",
      "450 time: 0:09:29.753930 g_loss: 23.505308151245117 d_loss: 0.05275914818048477\n",
      "460 time: 0:09:42.845619 g_loss: 33.17726135253906 d_loss: 0.00474721915088594\n",
      "470 time: 0:09:55.019196 g_loss: 29.818653106689453 d_loss: 0.0072592051001265645\n",
      "480 time: 0:10:07.170387 g_loss: 22.008682250976562 d_loss: 0.016330162471604126\n",
      "490 time: 0:10:19.344068 g_loss: 27.686662673950195 d_loss: 0.033019790367688984\n",
      "500 time: 0:10:31.485544 g_loss: 38.577117919921875 d_loss: 0.00825316330883652\n",
      "510 time: 0:10:44.735562 g_loss: 21.544645309448242 d_loss: 0.07359115855069831\n",
      "520 time: 0:10:56.842487 g_loss: 45.0394172668457 d_loss: 0.014361025532707572\n",
      "530 time: 0:11:08.967573 g_loss: 29.044374465942383 d_loss: 0.008065276546403766\n",
      "540 time: 0:11:21.098352 g_loss: 22.575387954711914 d_loss: 0.0012376339873299003\n",
      "550 time: 0:11:33.214270 g_loss: 36.66252517700195 d_loss: 0.07657370273955166\n",
      "560 time: 0:11:46.308278 g_loss: 13.123237609863281 d_loss: 0.020126589566643815\n",
      "570 time: 0:11:58.458293 g_loss: 33.03894805908203 d_loss: 0.012398844773997553\n",
      "580 time: 0:12:10.655716 g_loss: 41.65785598754883 d_loss: 0.3399314135313034\n",
      "590 time: 0:12:22.839906 g_loss: 21.361377716064453 d_loss: 0.0032148236641660333\n",
      "600 time: 0:12:34.985428 g_loss: 32.92182922363281 d_loss: 0.001826036546844989\n",
      "610 time: 0:12:48.022469 g_loss: 49.03252410888672 d_loss: 0.004016977967694402\n",
      "620 time: 0:13:00.148990 g_loss: 23.106229782104492 d_loss: 0.003317055990919471\n",
      "630 time: 0:13:12.244126 g_loss: 30.735122680664062 d_loss: 0.00046657609345857054\n",
      "640 time: 0:13:24.346970 g_loss: 35.18090057373047 d_loss: 0.1894076734315604\n",
      "650 time: 0:13:36.482106 g_loss: 32.93745803833008 d_loss: 0.002791281876852736\n",
      "660 time: 0:13:49.543298 g_loss: 34.18880844116211 d_loss: 0.008081065374426544\n",
      "670 time: 0:14:01.619077 g_loss: 62.120487213134766 d_loss: 0.007229219714645296\n",
      "680 time: 0:14:13.722501 g_loss: 34.665897369384766 d_loss: 0.001372172322589904\n",
      "690 time: 0:14:25.810162 g_loss: 19.74944305419922 d_loss: 0.0002825672272592783\n",
      "700 time: 0:14:37.892160 g_loss: 14.415986061096191 d_loss: 0.009059854783117771\n",
      "710 time: 0:14:50.877906 g_loss: 49.16854476928711 d_loss: 0.011115472530946136\n",
      "720 time: 0:15:02.933334 g_loss: 26.349273681640625 d_loss: 0.007618088944582269\n",
      "730 time: 0:15:14.992065 g_loss: 33.677528381347656 d_loss: 0.021406538551673293\n",
      "740 time: 0:15:27.053367 g_loss: 18.062349319458008 d_loss: 0.002130249748006463\n",
      "750 time: 0:15:39.381982 g_loss: 22.793569564819336 d_loss: 0.0024373307824134827\n",
      "760 time: 0:15:52.448418 g_loss: 43.0145378112793 d_loss: 0.0009957796428352594\n",
      "770 time: 0:16:04.561102 g_loss: 21.147777557373047 d_loss: 0.007749305354082026\n",
      "780 time: 0:16:16.744794 g_loss: 14.089964866638184 d_loss: 0.0006795399895054288\n",
      "790 time: 0:16:28.913382 g_loss: 19.77552604675293 d_loss: 0.0017912322073243558\n",
      "800 time: 0:16:41.068088 g_loss: 24.654048919677734 d_loss: 0.05134652741253376\n",
      "810 time: 0:16:54.122144 g_loss: 21.913362503051758 d_loss: 0.11269301423453726\n",
      "820 time: 0:17:06.177498 g_loss: 29.456890106201172 d_loss: 0.019275201542768627\n",
      "830 time: 0:17:18.291799 g_loss: 27.013334274291992 d_loss: 0.4001353154890239\n",
      "840 time: 0:17:30.452479 g_loss: 41.69321823120117 d_loss: 0.02087714382651029\n",
      "850 time: 0:17:42.565092 g_loss: 40.17097473144531 d_loss: 0.021779422357212752\n",
      "860 time: 0:17:55.574632 g_loss: 10.344306945800781 d_loss: 0.004549664881778881\n",
      "870 time: 0:18:07.674800 g_loss: 19.31224822998047 d_loss: 0.00023694487754255533\n",
      "880 time: 0:18:19.776559 g_loss: 23.088682174682617 d_loss: 0.3866048965137452\n",
      "890 time: 0:18:31.926835 g_loss: 15.385432243347168 d_loss: 0.0016361486705136485\n",
      "900 time: 0:18:44.064045 g_loss: 29.967329025268555 d_loss: 0.001591418171301484\n",
      "910 time: 0:18:57.124885 g_loss: 35.58763122558594 d_loss: 0.0421656513644848\n",
      "920 time: 0:19:09.466780 g_loss: 21.405405044555664 d_loss: 0.003130183555185795\n",
      "930 time: 0:19:21.543316 g_loss: 28.003345489501953 d_loss: 0.01524604787118733\n",
      "940 time: 0:19:33.633085 g_loss: 17.43018913269043 d_loss: 0.0005650513194268569\n",
      "950 time: 0:19:46.547265 g_loss: 14.021422386169434 d_loss: 0.0016661345434840769\n",
      "960 time: 0:19:59.718883 g_loss: 57.31382751464844 d_loss: 0.05970828964927932\n",
      "970 time: 0:20:11.789447 g_loss: 38.87879943847656 d_loss: 0.000352289411239326\n",
      "980 time: 0:20:23.895461 g_loss: 34.53288650512695 d_loss: 0.0003751367066797684\n",
      "990 time: 0:20:36.308689 g_loss: 31.64984703063965 d_loss: 0.003435226157307625\n",
      "1000 time: 0:20:48.408805 g_loss: 36.75614547729492 d_loss: 0.004391713999211788\n",
      "1010 time: 0:21:01.422315 g_loss: 35.42850112915039 d_loss: 0.0008341647717315936\n",
      "1020 time: 0:21:13.492502 g_loss: 26.22594451904297 d_loss: 0.0030271841897047125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1030 time: 0:21:25.595704 g_loss: 17.102270126342773 d_loss: 0.0005492285272339359\n",
      "1040 time: 0:21:37.707357 g_loss: 37.70618438720703 d_loss: 0.007794791570631787\n",
      "1050 time: 0:21:49.783192 g_loss: 20.927274703979492 d_loss: 0.3296257547335699\n",
      "1060 time: 0:22:02.827873 g_loss: 12.160476684570312 d_loss: 0.0001967746575246565\n",
      "1070 time: 0:22:14.935056 g_loss: 16.992748260498047 d_loss: 0.005221899375101202\n",
      "1080 time: 0:22:27.084902 g_loss: 31.913658142089844 d_loss: 0.0019283852889202535\n",
      "1090 time: 0:22:39.173964 g_loss: 24.314308166503906 d_loss: 0.08906803795616725\n",
      "1100 time: 0:22:51.265717 g_loss: 14.11486530303955 d_loss: 0.0012529421946965158\n",
      "1110 time: 0:23:04.262268 g_loss: 32.973960876464844 d_loss: 0.0010189711465500295\n",
      "1120 time: 0:23:16.330789 g_loss: 19.38916015625 d_loss: 0.05257016736504738\n",
      "1130 time: 0:23:28.448324 g_loss: 34.52748489379883 d_loss: 0.00018491544324206188\n",
      "1140 time: 0:23:40.477677 g_loss: 21.88515853881836 d_loss: 0.00022743943554814905\n",
      "1150 time: 0:23:52.549293 g_loss: 19.098968505859375 d_loss: 0.000601038453169167\n",
      "1160 time: 0:24:05.611348 g_loss: 25.128923416137695 d_loss: 0.2289958296896657\n",
      "1170 time: 0:24:17.736869 g_loss: 20.081478118896484 d_loss: 0.011533923068782315\n",
      "1180 time: 0:24:29.840904 g_loss: 16.252182006835938 d_loss: 0.0002450947358738631\n",
      "1190 time: 0:24:42.087957 g_loss: 35.029701232910156 d_loss: 0.01818338321390911\n",
      "1200 time: 0:24:54.202698 g_loss: 32.97430419921875 d_loss: 0.03183947457000613\n",
      "1210 time: 0:25:07.234871 g_loss: 30.382898330688477 d_loss: 0.0017755610460881144\n",
      "1220 time: 0:25:19.254903 g_loss: 28.21216583251953 d_loss: 0.2578881289809942\n",
      "1230 time: 0:25:31.303482 g_loss: 21.508333206176758 d_loss: 0.0056488011323381215\n",
      "1240 time: 0:25:43.364980 g_loss: 16.4765625 d_loss: 0.0033235332812182605\n",
      "1250 time: 0:25:55.413151 g_loss: 20.597564697265625 d_loss: 0.15042949467897415\n",
      "1260 time: 0:26:08.711010 g_loss: 17.783903121948242 d_loss: 0.0034953841459355317\n",
      "1270 time: 0:26:20.858116 g_loss: 21.20993423461914 d_loss: 0.002901636159094778\n",
      "1280 time: 0:26:32.990453 g_loss: 25.05194854736328 d_loss: 0.00026967463782057166\n",
      "1290 time: 0:26:45.180534 g_loss: 38.29985809326172 d_loss: 0.0006243798161449376\n",
      "1300 time: 0:26:57.332857 g_loss: 43.57965087890625 d_loss: 0.0012714670592686161\n",
      "1310 time: 0:27:10.453796 g_loss: 36.651004791259766 d_loss: 0.002483122196281329\n",
      "1320 time: 0:27:22.636556 g_loss: 14.338558197021484 d_loss: 0.00023788832913851365\n",
      "1330 time: 0:27:34.881561 g_loss: 22.872879028320312 d_loss: 0.0003524155035847798\n",
      "1340 time: 0:27:47.022255 g_loss: 26.286128997802734 d_loss: 0.010734897509337316\n",
      "1350 time: 0:27:59.204161 g_loss: 27.21751594543457 d_loss: 0.2592482225918502\n",
      "1360 time: 0:28:12.318732 g_loss: 17.49886703491211 d_loss: 0.005544171785004437\n",
      "1370 time: 0:28:24.464724 g_loss: 29.461597442626953 d_loss: 0.0510224011450191\n",
      "1380 time: 0:28:36.615440 g_loss: 22.333683013916016 d_loss: 0.012532350039691664\n",
      "1390 time: 0:28:48.753325 g_loss: 16.82966423034668 d_loss: 0.0006357331585604697\n",
      "1400 time: 0:29:00.845917 g_loss: 20.331018447875977 d_loss: 0.01668394444459409\n",
      "1410 time: 0:29:13.881589 g_loss: 15.664471626281738 d_loss: 0.003360695962328464\n",
      "1420 time: 0:29:25.982396 g_loss: 21.631689071655273 d_loss: 0.00144292222103104\n",
      "1430 time: 0:29:38.098247 g_loss: 25.137943267822266 d_loss: 0.0009572138078510761\n",
      "1440 time: 0:29:50.290282 g_loss: 21.502798080444336 d_loss: 0.0002683700331544969\n",
      "1450 time: 0:30:02.492844 g_loss: 13.976919174194336 d_loss: 0.00020080263493582606\n",
      "1460 time: 0:30:15.587814 g_loss: 17.92571449279785 d_loss: 0.003973163198679686\n",
      "1470 time: 0:30:27.739656 g_loss: 18.60323715209961 d_loss: 0.006874811137095094\n",
      "1480 time: 0:30:39.829781 g_loss: 36.128517150878906 d_loss: 0.45042629246017896\n",
      "1490 time: 0:30:51.946008 g_loss: 16.45535659790039 d_loss: 0.0026630862485035323\n",
      "1500 time: 0:31:04.126725 g_loss: 31.140369415283203 d_loss: 8.57136237755185e-05\n",
      "1510 time: 0:31:17.156370 g_loss: 12.646681785583496 d_loss: 0.005781407828180818\n",
      "1520 time: 0:31:29.216474 g_loss: 17.11362075805664 d_loss: 0.0012098696315661073\n",
      "1530 time: 0:31:41.289272 g_loss: 13.242141723632812 d_loss: 0.22529502189718187\n",
      "1540 time: 0:31:53.377985 g_loss: 19.857646942138672 d_loss: 0.03662493184674531\n",
      "1550 time: 0:32:05.497044 g_loss: 46.255287170410156 d_loss: 0.08363140560686588\n",
      "1560 time: 0:32:18.510964 g_loss: 22.64150047302246 d_loss: 0.0026950762839987874\n",
      "1570 time: 0:32:30.928140 g_loss: 13.830403327941895 d_loss: 0.0002200382514274679\n",
      "1580 time: 0:32:43.063081 g_loss: 13.289645195007324 d_loss: 0.0036286122631281614\n",
      "1590 time: 0:32:55.216267 g_loss: 24.185945510864258 d_loss: 0.0014636430441896664\n",
      "1600 time: 0:33:07.369174 g_loss: 13.77719783782959 d_loss: 0.002414861330180429\n",
      "1610 time: 0:33:20.413821 g_loss: 22.567766189575195 d_loss: 0.06315850553801283\n",
      "1620 time: 0:33:32.549043 g_loss: 17.54437828063965 d_loss: 0.000975329297943972\n",
      "1630 time: 0:33:44.649390 g_loss: 28.594118118286133 d_loss: 0.00200758414939628\n",
      "1640 time: 0:33:56.760662 g_loss: 40.48359680175781 d_loss: 0.002257320040371269\n",
      "1650 time: 0:34:08.872535 g_loss: 61.09998321533203 d_loss: 0.0002631459792610258\n",
      "1660 time: 0:34:21.927008 g_loss: 16.421417236328125 d_loss: 0.2967769652605057\n",
      "1670 time: 0:34:34.046083 g_loss: 18.489042282104492 d_loss: 0.06337040185462683\n",
      "1680 time: 0:34:46.224679 g_loss: 18.69240951538086 d_loss: 0.0007630884470017918\n",
      "1690 time: 0:34:58.356537 g_loss: 20.75681495666504 d_loss: 0.0017191914666909724\n",
      "1700 time: 0:35:10.535390 g_loss: 18.221712112426758 d_loss: 0.055809746271734184\n",
      "1710 time: 0:35:23.618835 g_loss: 18.339956283569336 d_loss: 0.010750416782684624\n",
      "1720 time: 0:35:35.692981 g_loss: 20.492839813232422 d_loss: 0.0002238552515336778\n",
      "1730 time: 0:35:47.800998 g_loss: 19.265819549560547 d_loss: 5.8846886531682685e-05\n",
      "1740 time: 0:35:59.907442 g_loss: 37.21751403808594 d_loss: 5.567866355704609e-05\n",
      "1750 time: 0:36:12.052537 g_loss: 29.788089752197266 d_loss: 4.7951894885045476e-05\n",
      "1760 time: 0:36:25.123320 g_loss: 20.592905044555664 d_loss: 0.0019871859876730014\n",
      "1770 time: 0:36:37.232824 g_loss: 28.602054595947266 d_loss: 0.0024048513841989916\n",
      "1780 time: 0:36:49.388697 g_loss: 20.165040969848633 d_loss: 0.0001745033951010555\n",
      "1790 time: 0:37:01.520797 g_loss: 47.685855865478516 d_loss: 3.830664991255617e-05\n",
      "1800 time: 0:37:13.685601 g_loss: 24.199371337890625 d_loss: 8.295439783978509e-05\n",
      "1810 time: 0:37:26.694891 g_loss: 50.726436614990234 d_loss: 0.0002491308841854334\n",
      "1820 time: 0:37:38.800660 g_loss: 39.87483215332031 d_loss: 0.00035523975839169\n",
      "1830 time: 0:37:50.922664 g_loss: 18.679248809814453 d_loss: 0.0013965775115138968\n",
      "1840 time: 0:38:03.021306 g_loss: 29.322879791259766 d_loss: 4.226866985845845e-05\n",
      "1850 time: 0:38:15.101713 g_loss: 56.963768005371094 d_loss: 0.00016058652136052842\n",
      "1860 time: 0:38:28.066506 g_loss: 17.149356842041016 d_loss: 0.0005976758475298993\n",
      "1870 time: 0:38:40.150715 g_loss: 27.442840576171875 d_loss: 7.586253559566103e-05\n",
      "1880 time: 0:38:52.220860 g_loss: 16.20659065246582 d_loss: 0.00027017100364901125\n",
      "1890 time: 0:39:04.306462 g_loss: 48.193729400634766 d_loss: 0.0015158019414229784\n",
      "1900 time: 0:39:16.408049 g_loss: 16.731855392456055 d_loss: 0.0005017788898840081\n",
      "1910 time: 0:39:29.501338 g_loss: 24.36432647705078 d_loss: 0.003689001399834524\n",
      "1920 time: 0:39:42.095919 g_loss: 18.98409652709961 d_loss: 3.411864963709377e-05\n",
      "1930 time: 0:39:54.315976 g_loss: 16.267099380493164 d_loss: 0.00020235503325238824\n",
      "1940 time: 0:40:06.526750 g_loss: 10.227319717407227 d_loss: 0.019607474436270422\n",
      "1950 time: 0:40:18.709604 g_loss: 39.82676696777344 d_loss: 0.010930165066383779\n",
      "1960 time: 0:40:31.842257 g_loss: 29.083993911743164 d_loss: 0.021200566552579403\n",
      "1970 time: 0:40:44.105298 g_loss: 21.7539005279541 d_loss: 0.43852075963513926\n",
      "1980 time: 0:40:56.253869 g_loss: 21.94936752319336 d_loss: 0.0030797700164839625\n",
      "1990 time: 0:41:08.406009 g_loss: 42.22883605957031 d_loss: 0.001420753716956824\n",
      "2000 time: 0:41:20.758427 g_loss: 15.85962200164795 d_loss: 0.0013014456126256846\n",
      "2010 time: 0:41:33.915942 g_loss: 23.53858184814453 d_loss: 0.0012335988867562264\n",
      "2020 time: 0:41:46.093079 g_loss: 20.540170669555664 d_loss: 0.0001904443524836097\n",
      "2030 time: 0:41:58.243285 g_loss: 15.628804206848145 d_loss: 0.00018383912538411096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2040 time: 0:42:10.400305 g_loss: 26.091428756713867 d_loss: 0.04544722006539814\n",
      "2050 time: 0:42:22.543933 g_loss: 21.452810287475586 d_loss: 0.00012579606845974922\n",
      "2060 time: 0:42:35.630767 g_loss: 16.85350799560547 d_loss: 0.00018598911083245184\n",
      "2070 time: 0:42:47.748754 g_loss: 27.578283309936523 d_loss: 0.0006790529587306082\n",
      "2080 time: 0:42:59.896348 g_loss: 29.074953079223633 d_loss: 0.00010011155609390698\n",
      "2090 time: 0:43:11.996436 g_loss: 20.804988861083984 d_loss: 0.007706864969804883\n",
      "2100 time: 0:43:24.090588 g_loss: 32.778377532958984 d_loss: 0.00011132494182675146\n",
      "2110 time: 0:43:37.250811 g_loss: 17.914865493774414 d_loss: 0.0006666344415862113\n",
      "2120 time: 0:43:49.348163 g_loss: 28.24638557434082 d_loss: 0.0003240539226680994\n",
      "2130 time: 0:44:01.449234 g_loss: 8.326123237609863 d_loss: 0.1367810462070338\n",
      "2140 time: 0:44:13.532745 g_loss: 10.642097473144531 d_loss: 0.001668968383455649\n",
      "2150 time: 0:44:25.625862 g_loss: 31.04475975036621 d_loss: 0.0004169304847891908\n",
      "2160 time: 0:44:38.716736 g_loss: 26.52049446105957 d_loss: 0.010899829401751049\n",
      "2170 time: 0:44:50.813031 g_loss: 22.16903305053711 d_loss: 0.0012681389548561128\n",
      "2180 time: 0:45:02.908949 g_loss: 14.801008224487305 d_loss: 0.009416625733138062\n",
      "2190 time: 0:45:14.974536 g_loss: 27.461156845092773 d_loss: 0.0026566302694845945\n",
      "2200 time: 0:45:27.141791 g_loss: 15.491863250732422 d_loss: 8.225462988775689e-05\n",
      "2210 time: 0:45:40.218306 g_loss: 11.851374626159668 d_loss: 4.046294770887471e-05\n",
      "2220 time: 0:45:52.354725 g_loss: 16.243173599243164 d_loss: 0.0003947047080146149\n",
      "2230 time: 0:46:04.472128 g_loss: 26.229272842407227 d_loss: 0.0011138054069306236\n",
      "2240 time: 0:46:16.559318 g_loss: 20.583730697631836 d_loss: 2.5231709514628164e-05\n",
      "2250 time: 0:46:28.640795 g_loss: 32.623722076416016 d_loss: 9.109911661653314e-05\n",
      "2260 time: 0:46:41.620910 g_loss: 13.060345649719238 d_loss: 0.0002900914819292666\n",
      "2270 time: 0:46:53.738482 g_loss: 24.133583068847656 d_loss: 0.000555391248781234\n",
      "2280 time: 0:47:05.759972 g_loss: 28.947866439819336 d_loss: 0.00012109425915696193\n",
      "2290 time: 0:47:17.823632 g_loss: 21.680774688720703 d_loss: 7.88083411862317e-05\n",
      "2300 time: 0:47:29.905861 g_loss: 13.705353736877441 d_loss: 6.074122666177573e-05\n",
      "2310 time: 0:47:43.362454 g_loss: 14.73754596710205 d_loss: 0.00029543485106842127\n",
      "2320 time: 0:47:55.560316 g_loss: 29.930706024169922 d_loss: 0.0010291059104474698\n",
      "2330 time: 0:48:07.844020 g_loss: 16.95664405822754 d_loss: 7.232522909816907e-05\n",
      "2340 time: 0:48:20.045153 g_loss: 21.541000366210938 d_loss: 5.369305199565133e-05\n",
      "2350 time: 0:48:32.255770 g_loss: 27.22579574584961 d_loss: 0.00019224675997975282\n",
      "2360 time: 0:48:45.370082 g_loss: 43.28948974609375 d_loss: 7.948506754473783e-05\n",
      "2370 time: 0:48:57.536251 g_loss: 16.806697845458984 d_loss: 2.1397216187324375e-05\n",
      "2380 time: 0:49:09.737432 g_loss: 15.079902648925781 d_loss: 0.07596342153556179\n",
      "2390 time: 0:49:21.919984 g_loss: 12.638739585876465 d_loss: 0.0007888449763413519\n",
      "2400 time: 0:49:34.092325 g_loss: 15.40938663482666 d_loss: 0.0005157317209523171\n",
      "2410 time: 0:49:47.160465 g_loss: 10.47716236114502 d_loss: 0.01792208506958559\n",
      "2420 time: 0:49:59.273690 g_loss: 16.336381912231445 d_loss: 0.00011995659951935522\n",
      "2430 time: 0:50:11.410058 g_loss: 20.156917572021484 d_loss: 0.13599321800938924\n",
      "2440 time: 0:50:23.541648 g_loss: 27.526098251342773 d_loss: 0.0004631962801795453\n",
      "2450 time: 0:50:35.645842 g_loss: 15.690730094909668 d_loss: 0.0030871499639033573\n",
      "2460 time: 0:50:48.692764 g_loss: 24.48191261291504 d_loss: 0.00043119312613271177\n",
      "2470 time: 0:51:00.866425 g_loss: 21.78776741027832 d_loss: 4.6933757403166965e-05\n",
      "2480 time: 0:51:13.049055 g_loss: 16.798484802246094 d_loss: 0.002488969721525791\n",
      "2490 time: 0:51:25.187127 g_loss: 15.206886291503906 d_loss: 0.0004794042206412996\n",
      "2500 time: 0:51:37.312721 g_loss: 30.1240234375 d_loss: 0.0011019892108379281\n",
      "2510 time: 0:51:50.345954 g_loss: 22.257177352905273 d_loss: 1.4273595752456458e-05\n",
      "2520 time: 0:52:02.476508 g_loss: 28.838544845581055 d_loss: 0.00036713963163492735\n",
      "2530 time: 0:52:14.572056 g_loss: 21.606399536132812 d_loss: 4.519893991528079e-05\n",
      "2540 time: 0:52:26.650469 g_loss: 21.27849769592285 d_loss: 0.00045698644657932164\n",
      "2550 time: 0:52:38.762256 g_loss: 14.244721412658691 d_loss: 0.116921859793365\n",
      "2560 time: 0:52:51.748553 g_loss: 12.729375839233398 d_loss: 0.00034949718974530697\n",
      "2570 time: 0:53:03.794790 g_loss: 38.75507736206055 d_loss: 0.0037608298589475453\n",
      "2580 time: 0:53:15.890802 g_loss: 30.608150482177734 d_loss: 0.0005835052943439223\n",
      "2590 time: 0:53:27.947099 g_loss: 12.48755168914795 d_loss: 0.0010234929141006432\n",
      "2600 time: 0:53:40.042661 g_loss: 21.543285369873047 d_loss: 0.09887406034522428\n",
      "2610 time: 0:53:53.044116 g_loss: 13.494503021240234 d_loss: 0.0019659850804600865\n",
      "2620 time: 0:54:05.170954 g_loss: 18.2834529876709 d_loss: 0.015162470846917131\n",
      "2630 time: 0:54:17.268290 g_loss: 17.454225540161133 d_loss: 0.25496387481689453\n",
      "2640 time: 0:54:29.338025 g_loss: 40.50290298461914 d_loss: 0.00015557671213173307\n",
      "2650 time: 0:54:41.433243 g_loss: 28.316499710083008 d_loss: 0.016803201549919322\n",
      "2660 time: 0:54:54.429664 g_loss: 16.34735870361328 d_loss: 0.3285000147880055\n",
      "2670 time: 0:55:06.565194 g_loss: 15.962642669677734 d_loss: 0.0022856278983454104\n",
      "2680 time: 0:55:18.638220 g_loss: 43.913692474365234 d_loss: 0.012329484801739454\n",
      "2690 time: 0:55:30.703489 g_loss: 16.333070755004883 d_loss: 0.009944120516593102\n",
      "2700 time: 0:55:42.850763 g_loss: 10.190363883972168 d_loss: 0.1459821714888676\n",
      "2710 time: 0:55:55.857713 g_loss: 14.78380012512207 d_loss: 7.957837442518212e-05\n",
      "2720 time: 0:56:07.915585 g_loss: 23.637943267822266 d_loss: 0.026435637002578005\n",
      "2730 time: 0:56:20.024037 g_loss: 13.562158584594727 d_loss: 0.0006484017212642357\n",
      "2740 time: 0:56:32.112592 g_loss: 29.232158660888672 d_loss: 0.0003099136520177126\n",
      "2750 time: 0:56:44.195590 g_loss: 25.757375717163086 d_loss: 0.0005267033411655575\n",
      "2760 time: 0:56:57.857044 g_loss: 32.064598083496094 d_loss: 0.00018387051022727974\n",
      "2770 time: 0:57:10.071286 g_loss: 20.60332489013672 d_loss: 5.674721069226507e-05\n",
      "2780 time: 0:57:22.303072 g_loss: 14.377036094665527 d_loss: 0.0018440868007019162\n",
      "2790 time: 0:57:34.545576 g_loss: 22.73086929321289 d_loss: 0.11810681704082526\n",
      "2800 time: 0:57:46.742852 g_loss: 34.56874084472656 d_loss: 0.00016021029193780123\n",
      "2810 time: 0:57:59.871026 g_loss: 25.546863555908203 d_loss: 0.0006323828165477607\n",
      "2820 time: 0:58:12.021898 g_loss: 15.287834167480469 d_loss: 0.007060167215968249\n",
      "2830 time: 0:58:24.189416 g_loss: 10.883864402770996 d_loss: 4.37995595348184e-05\n",
      "2840 time: 0:58:36.406769 g_loss: 16.26006507873535 d_loss: 1.8074834770231973e-05\n",
      "2850 time: 0:58:48.573848 g_loss: 15.218338966369629 d_loss: 0.0002821657108142972\n",
      "2860 time: 0:59:01.615010 g_loss: 17.95983123779297 d_loss: 0.02603475924865961\n",
      "2870 time: 0:59:13.753074 g_loss: 19.047636032104492 d_loss: 0.12300216482799442\n",
      "2880 time: 0:59:25.879081 g_loss: 15.287322998046875 d_loss: 0.0017336952660116367\n",
      "2890 time: 0:59:38.072014 g_loss: 16.47667121887207 d_loss: 0.00018620299306348898\n",
      "2900 time: 0:59:50.231460 g_loss: 10.04111385345459 d_loss: 0.004529030384901489\n",
      "2910 time: 1:00:03.327030 g_loss: 15.849137306213379 d_loss: 0.0002801459122565575\n",
      "2920 time: 1:00:15.475548 g_loss: 11.42011547088623 d_loss: 6.359388862620108e-05\n",
      "2930 time: 1:00:27.610927 g_loss: 20.775842666625977 d_loss: 0.0004044783636345528\n",
      "2940 time: 1:00:39.753390 g_loss: 24.70057487487793 d_loss: 0.0002458859180478612\n",
      "2950 time: 1:00:51.906012 g_loss: 34.77470779418945 d_loss: 0.00291762116830796\n",
      "2960 time: 1:01:04.975694 g_loss: 9.115959167480469 d_loss: 0.007165207171055954\n",
      "2970 time: 1:01:17.160870 g_loss: 36.250553131103516 d_loss: 7.304533505703148e-05\n",
      "2980 time: 1:01:29.310050 g_loss: 14.222528457641602 d_loss: 0.000548681824511732\n",
      "2990 time: 1:01:41.442699 g_loss: 19.357572555541992 d_loss: 0.0006179840347613208\n",
      "3000 time: 1:01:53.556751 g_loss: 11.71594524383545 d_loss: 0.002201360068283975\n",
      "INFO:tensorflow:Assets written to: ./saved_model/teacher_epoch3000/assets\n"
     ]
    }
   ],
   "source": [
    "gan = SRGAN()\n",
    "gan.train(epochs=4000, batch_size=1, sample_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'first_baseline_epoch4000_model'\n",
    "gan.generator.save('./saved_model/%s' % model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 256, 256, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 256, 256, 64)      1792      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 256, 256, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 128, 128, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 128, 128, 128)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128, 128, 128)     512       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 64, 64, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 64, 64, 256)       295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 64, 64, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64, 64, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32, 32, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 32, 32, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 16, 16, 512)       2048      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16, 16, 1024)      525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 16, 16, 1024)      0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16, 16, 1)         1025      \n",
      "=================================================================\n",
      "Total params: 5,219,137\n",
      "Trainable params: 0\n",
      "Non-trainable params: 5,219,137\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan = SRGAN()\n",
    "gan.discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
