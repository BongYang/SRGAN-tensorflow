{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refine Myself by Teaching Myself : Feature Refinement via Self-Knowledge Distillation\n",
    "\n",
    "#### Mingi Ji, Seungjae Shin, Seunghyun Hwang, Gibeom Park, Il-Chul Moon (KAIST) \n",
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "#### Knowledge distillationd이란? \n",
    "    Pre-train된 Teacher(용량이 큰 모델)의 Knowledge를 student(용량을 줄인 모델)로 transfer하는 방법이다\n",
    "\n",
    "#### 그 중 Self knowledge distillation이란?\n",
    "    Student를 teacher없이 점진적으로 distillation하는 방법이다\n",
    "\n",
    "\n",
    "    Self knowledge distillation에는 Data augmentation based approach와 auxiliary self network based approach가 존재한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__이 논문에서는 auxiliary self teacher network를 활용한 novel self knowledge distillation mothod를 제시한다고 한다__\n",
    "\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowldge distillation이 성공적이었음에도 불구하고    \n",
    "일반적인 Knowledge distillation에서는 Pre-trained teacher를 사용하는데, 이 Teacher를 학습시키는데에 자원이 너무 많이 소요된다는 문제점이 있다.\n",
    "그리고 Teacher가 변함에 따라서 student의 성능이 달라질 수 있다.\n",
    "       \n",
    "그러나 Self knowledge distillation에서는 pre-trained teacher없이 Knowledge distillation을 수행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self knowledge distillatoin에는 두가지 방법이 있다\n",
    "\n",
    "#### 1. Data augmentation based approach\n",
    "    하나의 데이터 인스턴스에 서로 다른 Augmentation을 적용하여 같은 network를 통과시켜 그 두개의 Output을 같게 하는 방식으로 Self distillation을 수행\n",
    "\n",
    "    단점 : Augmentation과정에서 local information이 줄어든다 -> feature distillation을 적용하기 힘들다\n",
    "          다양한 Vision Task에 적용이 어렵다\n",
    "          Refined feature map을 받지 못한다. (Object detection과 Semantic segmentation에서는 refined feature map이 중요하다)\n",
    "      \n",
    "#### 2. Auxiliary network based approach\n",
    "    Classifier network 중간에 추가적인 Branch를 두어 그 Branch에서 나온 결과가 같아지도록 Knowledge distillation을 수행\n",
    "\n",
    "    단점 : Auxiliary network에 의존한다.\n",
    "          Refined knowledge를 생성하기 힘들다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 방법들의 단점들을 보완하기 위해 이 논문에서는 Feature Refinement via Self-Knowledge Distillation(FRSKD)를 수행함       \n",
    "FRSKD는 auxiliary network가 refined knowledge까지 전달할 수 있고 Classification과 Semantic segmentation에도 적용이 가능하다고 한다 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://images.velog.io/images/douvlecircle/post/26c5c479-3c80-40ff-95de-d275b21d4666/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202022-02-12%20%E1%84%8B%E1%85%A9%E1%84%8C%E1%85%A5%E1%86%AB%2012.16.30.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
