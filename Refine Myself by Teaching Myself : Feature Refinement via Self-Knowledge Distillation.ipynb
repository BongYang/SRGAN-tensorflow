{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refine Myself by Teaching Myself : Feature Refinement via Self-Knowledge Distillation\n",
    "\n",
    "#### Mingi Ji, Seungjae Shin, Seunghyun Hwang, Gibeom Park, Il-Chul Moon (KAIST) \n",
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "#### Knowledge distillationd이란? \n",
    "    Pre-train된 Teacher(용량이 큰 모델)의 Knowledge를 student(용량을 줄인 모델)로 transfer하는 방법이다\n",
    "\n",
    "#### 그 중 Self knowledge distillation이란?\n",
    "    Student를 teacher없이 점진적으로 distillation하는 방법이다\n",
    "\n",
    "\n",
    "Self knowledge distillation에는 Data augmentation based approach와 auxiliary self network based approach가 존재한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__이 논문에서는 auxiliary self teacher network를 활용한 novel self knowledge distillation mothod를 제시한다고 한다__\n",
    "\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowldge distillation이 성공적이었음에도 불구하고 일반적인 Knowledge distillation에서는 Pre-trained teacher를 사용하는데,       \n",
    "이 Teacher를 학습시키는데에 자원이 많이 소요되고 학습된 Teacher에 따라 Student의 성능이 달라질 수 있다는 문제점이 있다.      \n",
    "\n",
    "그러나 Self knowledge distillation에서는 pre-trained teacher없이 Knowledge distillation을 수행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self knowledge distillatoin에는 두가지 방법이 있다\n",
    "\n",
    "##### 1. Data augmentation based approach\n",
    "    하나의 데이터 인스턴스에 서로 다른 Augmentation을 적용하여 같은 network를 통과시켜 그 두개의 Output을 같게 하는 방식으로 Self distillation\n",
    "\n",
    "    단점 : Augmentation과정에서 local information이 줄어든다 -> feature distillation을 적용하기 힘들다\n",
    "          다양한 Vision Task에 적용이 어렵다\n",
    "          Refined feature map을 받지 못한다. (Object detection과 Semantic segmentation에서는 refined feature map이 중요하다)\n",
    "      \n",
    "##### 2. Auxiliary network based approach\n",
    "    Classifier network 중간에 추가적인 Branch를 두어 그 Branch에서 나온 결과가 같아지도록 Knowledge distillation\n",
    "\n",
    "    단점 : Auxiliary network에 의존한다.\n",
    "          Refined knowledge를 생성하기 힘들다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__이 방법들의 단점들을 보완하기 위해 이 논문에서는 Feature Refinement via Self-Knowledge Distillation(FRSKD)를 수행함__      \n",
    "__FRSKD는 auxiliary network가 refined knowledge까지 전달할 수 있고 Classification과 Semantic segmentation에도 적용이 가능하다고 한다__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://images.velog.io/images/douvlecircle/post/26c5c479-3c80-40ff-95de-d275b21d4666/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202022-02-12%20%E1%84%8B%E1%85%A9%E1%84%8C%E1%85%A5%E1%86%AB%2012.16.30.png)\n",
    "\n",
    "위 그림에서 볼 수 있듯 FRSKD는 self teacher network를 사용하여 Distillation을 위한 Refined feature를 얻어냄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "self teacher network를 이용하여 Refined feature map과 soft label을 생성하도록 함\n",
    "\n",
    "self teacher network의 input으로는 classifier network의 feature map을 사용\n",
    "\n",
    "self teacher network의 구조는 BiFPN의 구조를 변형해서 사용\n",
    "\n",
    "PANet과 BiFPN의 top-down path, bottom-up path 적용\n",
    "\n",
    "top-down path에 들어가기 전에는 lateral conv layer를 활용\n",
    "\n",
    "해당 과정에서 생성된 Featuer map과 Soft label을 이용하여 Distillation에 사용함\n",
    "\n",
    "자세한 그림은 아래와 같음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://images.velog.io/images/douvlecircle/post/b167603c-38d0-4be6-897c-da8fc2608bae/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202022-02-14%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%204.28.31.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "해당 아이디어로 Self distilltion했을 시, Classification 문제와 Semantic segmentation에 좋은 결과를 보여줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self teacher와 classifier의 attention map 비교\n",
    "![](https://images.velog.io/images/douvlecircle/post/a807157d-1d99-478c-8c9c-0f47fbbbf52d/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202022-02-14%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%204.38.59.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 비교 결과\n",
    "![](https://images.velog.io/images/douvlecircle/post/a512a20b-8faf-4ae7-8f28-b08f5e79158b/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202022-02-14%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%204.42.10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Top-down, bottom-up path를 도입하여 self distillation을 적용함\n",
    "\n",
    "이를 통해 Refined feature를 얻음으로써 distillation에 더 좋은 결과를 얻어냄\n",
    "\n",
    "Classfication뿐만 아니라 Semantic segmentation에서도 좋은 결과를 얻음\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
