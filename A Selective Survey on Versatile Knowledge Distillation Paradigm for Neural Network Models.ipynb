{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Selective Survey on Versatile Knowledge Distillation Paradigm for Neural Network Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "- Section II - Distilled knowledge, Loss, Teacher-Student Architecture, Distillation process   \n",
    "   \n",
    "- Section III - Versatile KD - Vision, NLP, Quantization and so on..   \n",
    "   \n",
    "- Section IV - Supervised, Weakly Supervised, Semi/Un-Supervised Learning   \n",
    "    \n",
    "- Section V -    \n",
    "   \n",
    "- Section VI - Future Works\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section II\n",
    "--------------------\n",
    "### A. Distilled knowledge\n",
    "___Intial Works___    \n",
    "C.bucilua - 개념 처음으로 제안 Knowledge Transfer for compression    \n",
    "Hiton - KD 개념 대중화 - 더 일반적 Case + temparature 개념 + Classification Task\n",
    "\n",
    "\n",
    "그러나 KD의 성공에도 불구하고 Deep Neural Network에서 Teacher의 Knowledge가 어디에 존재하는지 or 그 Knowledge를 Capture하는 Optimal method는 여전히 의문으로 남아있음\n",
    "\n",
    "Loss가 Knowledge distillation의 Key factor임\n",
    "    \n",
    "     \n",
    "___Response Distillation___\n",
    "1. C.bucilua - teacher의 logit을 사용 -> Squared difference를 최소화   \n",
    "    \n",
    "2. Hinton - KL Divergence를 최소화 -> Temperature를 이용한 softened softmax output을 사용       \n",
    "            Softened output을 ___Dark Knowledge라 명칭___ <- 중요하 정보 (높은 엔트로피)      \n",
    "     \n",
    "    \n",
    "___Representation Space distillation___      \n",
    "Teacher의 Latent feature space를 모방      \n",
    "     \n",
    "Teacher의 hidden layer를 사용하여 KD 수행      \n",
    "그러나 Teacher와 Student 사이의 hidden layer의 차원의 수가 달라서 주의를 기울여야함       \n",
    "       \n",
    "Byengho - 1x1 conv를 사용하여 채널수를 조정 - novel feature distillation     \n",
    "       \n",
    "      \n",
    "--------------------\n",
    "### B. Teacher-Student Architecture\n",
    "\n",
    "__1. Single Teacher - Single Student__ \n",
    "    Teacher의 Knowledge를 옮기는 가장 Simple 방법    \n",
    "    그러나 Teacher와 Student사이의 Capacity gap이 너무 크면 KD의 성능이 떨어짐\n",
    "            \n",
    "<br>\n",
    "\n",
    "__2. Multi-step learning__      \n",
    "    Problem : Teacher와 Student사이의 Capacity gap이 크면 성능이 떨어지는 문제를 해결하기 위하 방법     \n",
    "    순차적으로 Capacity 차이를 줄여가면서 KD 수행\n",
    "\n",
    "   <br>       \n",
    "3. Multi-teacher learning     \n",
    "     Shan.You - thin and deep network를 학습시키기 위해 multiple teacher를 이용\n",
    "\n",
    "     3개의 Loss를 사용 - Dark Knowledge, Label Prediction, Relative similarity loss\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "--------------------       \n",
    "\n",
    "### C. Distillation Process\n",
    "\n",
    "1. offline Distillation     \n",
    "    Pretrained teacher -> transfer student\n",
    "\n",
    "\n",
    "2. Online Distillation\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
